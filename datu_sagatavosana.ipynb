{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxG_uNQOCpzP"
      },
      "source": [
        "## Teksta sarežģītības līmeņa klasifikācija"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OTCjpJ_lFGz"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U datasets\n",
        "!pip -q install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIYyVaZ-7DfU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import stanza\n",
        "import math\n",
        "import random\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHnPHJot7E5z"
      },
      "outputs": [],
      "source": [
        "!wget -q -O exam_texts.json https://raw.githubusercontent.com/LUMII-AILab/VTI-Data/refs/heads/main/lv-exams/NLG/VISC_LATV.json\n",
        "!wget -q -O essays.csv https://lava.korpuss.lv/download/essays_20211214.csv\n",
        "!wget -q -O top_words.csv https://raw.githubusercontent.com/IvoDz/lv-text-complexity/refs/heads/main/wordlist_LVK2022.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONq6jY6c40f"
      },
      "source": [
        "Sākotnējais teksta sarežģītības novērtēšanas algoritms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuMaSFVGa7dy"
      },
      "outputs": [],
      "source": [
        "words_df = pd.read_csv(\"top_words.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEcMz4FCAEIx"
      },
      "outputs": [],
      "source": [
        "nlp = stanza.Pipeline(lang='lv', processors='tokenize')\n",
        "\n",
        "word_freq = {}\n",
        "MAX_LOGFREQ = 1.0\n",
        "\n",
        "def init_classifier(words_df: pd.DataFrame):\n",
        "    \"\"\" Startē klasifikatoru, uzstrāda vārdu biežumus \"\"\"\n",
        "    global word_freq, MAX_LOGFREQ\n",
        "    word_freq = dict(words_df.values)\n",
        "    MAX_LOGFREQ = math.log10(max(word_freq.values(), default=1) + 1)\n",
        "\n",
        "def tokenize_words_and_sentences(text: str):\n",
        "    \"\"\" Sadala tekstu vārdos un tekstvienībās \"\"\"\n",
        "    doc = nlp(text)\n",
        "    words = [word.text.lower() for sent in doc.sentences for word in sent.words if word.text.isalpha()]\n",
        "    sents = [sent.text for sent in doc.sentences]\n",
        "    return words, sents\n",
        "\n",
        "def word_rarity(word: str) -> float:\n",
        "    \"\"\" Iegūst relatīvu vārda biežuma rādītāju vienam vārdam \"\"\"\n",
        "    freq = word_freq.get(word.lower(), 1)\n",
        "    log_freq = math.log10(freq + 1)\n",
        "    return max(0.0, 1.0 - log_freq / MAX_LOGFREQ)\n",
        "\n",
        "def avg_word_rarity(words: list[str]) -> float:\n",
        "    \"\"\" Iegūst vidēju relatīvu vārda biežuma rādītāju vārdu sarakstā \"\"\"\n",
        "    content = [w for w in words if len(w) > 3]\n",
        "    return (sum(word_rarity(w) for w in content) / len(content)) if content else 0.0\n",
        "\n",
        "def classify(text: str, debug: bool=False) -> dict:\n",
        "    \"\"\"\n",
        "    Klasificē tekstu vienā no 3 kategorijām: viegls, vidējs, sarežģīts\n",
        "        - ja vārds satur ne vairāk kā 5 vārdus un katrs vārds ir vai nu <= 3 simbolus garš, vai nu ir iekš top 1000 biežākajiem vārdiem - uzreiz atgriež \"viegls\"\n",
        "        - citādi, ja vārds ir ne vairāk kā 6 vārdus garš, atgriež rezultātu balstoties uz statiskiem parametriem vidējam vārda garumam un biežumam\n",
        "        - ja teikums ir garāks, rēķina rezultātu balstoties uz 4 parametriem:\n",
        "            - vid. vārda garums teikumā\n",
        "            - vid. vārdu retums sarakstā (no LVK biežumvārdnīcas)\n",
        "            - vid. teikuma garums tekstā\n",
        "            - vid. vārdu skaits teikumā\n",
        "     \"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return {\"text\": text, \"level\": \"nederīgs\", \"score\": None}\n",
        "\n",
        "    words, sents = tokenize_words_and_sentences(text)\n",
        "    if not words:\n",
        "        return {\"text\": text, \"level\": \"nederīgs\", \"score\": None}\n",
        "\n",
        "    wc = len(words)\n",
        "    avg_wlen = sum(len(w) for w in words) / wc\n",
        "    avg_slen = sum(len(sent.split()) for sent in sents if sent.strip()) / len(sents)\n",
        "    rarity = avg_word_rarity(words)\n",
        "    length_score = max(0.0, min((wc - 2) / 18.0, 1.0))\n",
        "\n",
        "    if wc <= 5 and all(word_freq.get(w, 0) > 1000 or len(w) <= 3 for w in words):\n",
        "        return {\"text\": text, \"level\": \"viegls\", \"score\": 0.0} if debug else {\"text\": text, \"level\": \"viegls\"}\n",
        "\n",
        "    if wc <= 6:\n",
        "        if avg_wlen <= 6.0:\n",
        "            lvl = \"viegls\"\n",
        "        elif avg_wlen <= 7.5:\n",
        "            lvl = \"vidējs\"\n",
        "        else:\n",
        "            lvl = \"vidējs\" if rarity < 0.5 else \"sarežģīts\"\n",
        "        return {\"text\": text, \"level\": lvl} if not debug else {\"text\": text, \"level\": lvl, \"score\": None}\n",
        "\n",
        "    score = (\n",
        "        0.35 * min(avg_wlen / 7.0, 1.0) +\n",
        "        0.2  * rarity +\n",
        "        0.25 * min(avg_slen / 20.0, 1.0) +\n",
        "        0.2  * length_score\n",
        "    )\n",
        "\n",
        "    if score < 0.5:\n",
        "        lvl = \"viegls\"\n",
        "    elif score < 0.6:\n",
        "        lvl = \"vidējs\"\n",
        "    else:\n",
        "        lvl = \"sarežģīts\"\n",
        "\n",
        "    result = {\"text\": text, \"level\": lvl}\n",
        "    if debug:\n",
        "        result.update({\n",
        "            \"score\": round(score, 4),\n",
        "            \"word_count\": wc,\n",
        "            \"avg_word_length\": round(avg_wlen, 2),\n",
        "            \"avg_sentence_length\": round(avg_slen, 2),\n",
        "            \"rarity\": round(rarity, 4),\n",
        "            \"length_score\": round(length_score, 4),\n",
        "        })\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_classifier(words_df) # inicializē klasifikatoru"
      ],
      "metadata": {
        "id": "gVCnvQLs-nb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9AUXhCkc-2R"
      },
      "source": [
        "Eseju tekstu sagatavošana / priekšapstrāde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypO4tX8Xd0qf"
      },
      "outputs": [],
      "source": [
        "def slice_from_second_sentence(text):\n",
        "    \"\"\" Atgriež teksta fragmentu, ne garāku par 20 vārdiem, sākot ar 2. teikumu dotajā tekstā \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    if len(sentences) < 2:\n",
        "        return None\n",
        "\n",
        "    start_idx = random.randint(2, len(sentences) - 1)\n",
        "\n",
        "    result_sentences = []\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in sentences[start_idx:]:\n",
        "        sentence = sentence.strip()\n",
        "        word_count = len(sentence.split())\n",
        "\n",
        "        if total_words + word_count > 20:\n",
        "            break\n",
        "\n",
        "        result_sentences.append(sentence)\n",
        "        total_words += word_count\n",
        "\n",
        "    if total_words < 2:\n",
        "        return None\n",
        "\n",
        "    return ' '.join(result_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZqAgV6VsOmpw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('essays.csv', usecols=['corrected_text'])\n",
        "df['corrected_text'] = (\n",
        "    df['corrected_text']\n",
        "    .astype(str)\n",
        "    .str.replace(r'\\r?\\n', ' ', regex=True)\n",
        "    .str.replace(r'\\*', '', regex=True)\n",
        "    .str.strip()\n",
        "    .str.replace(r'\\s+', ' ', regex=True)\n",
        ")\n",
        "\n",
        "\n",
        "df = df[df['corrected_text'].str.split().str.len() >= 2]\n",
        "df['char_len'] = df['corrected_text'].str.len()\n",
        "df = df.sort_values(by='char_len').head(350).copy()\n",
        "df = df.drop(columns='char_len')\n",
        "\n",
        "df['corrected_text'] = df['corrected_text'].apply(slice_from_second_sentence)\n",
        "df = df.dropna().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww0W1eCQH9dI"
      },
      "outputs": [],
      "source": [
        "classified_essays = df['corrected_text'].apply(classify)\n",
        "classified_essays = pd.DataFrame(classified_essays.tolist())\n",
        "classified_essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYfV_ilEnVet"
      },
      "outputs": [],
      "source": [
        "classified_essays.to_csv(\"essays_labeled.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVWTl-NJd_ja"
      },
      "source": [
        "Wikipedia tekstu sagatavošana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE-0CAvOh53D"
      },
      "outputs": [],
      "source": [
        "### ņemts no https://huggingface.co/datasets/RaivisDejus/latvian-text/blob/main/tools/wikipedia/GetWikipedia.py\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATE = \"20221120\"\n",
        "CUTOFF_SECTIONS = ['Atsauces un piezīmes', 'Atsauces', 'Ārējās saites', 'Literatūra', 'Skatīt arī',\n",
        "                   ' Atsauces un piezīmes', ' Atsauces', ' Ārējās saites', ' Literatūra', ' Skatīt arī']\n",
        "\n",
        "dataset = load_dataset('joelito/EU_Wikipedias', date=DATE, language=\"lv\", split='train', trust_remote_code=True)\n",
        "subset = dataset.select(range(10000))\n",
        "\n",
        "with open(f'wikipedia_{DATE}.txt', 'w', encoding='utf-8') as file:\n",
        "    for entry in tqdm(subset):\n",
        "        cutoffs = [len(entry[\"text\"])]\n",
        "        for section in CUTOFF_SECTIONS:\n",
        "            idx = entry[\"text\"].find('\\n\\n' + section)\n",
        "            if idx != -1:\n",
        "                cutoffs.append(idx)\n",
        "\n",
        "        file.write(f'{entry[\"title\"]}\\n\\n')\n",
        "        file.write(f'{entry[\"text\"][:min(cutoffs)]}\\n\\n\\n')\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAr54qnnn-T9"
      },
      "outputs": [],
      "source": [
        "with open('wikipedia_20221120.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
        "long_enough_lines = [line for line in non_empty_lines if len(line.split()) >= 10]\n",
        "\n",
        "sampled_lines = pd.Series(long_enough_lines).sample(\n",
        "    n=min(5000, len(long_enough_lines)),\n",
        "    random_state=42\n",
        ").reset_index(drop=True)\n",
        "\n",
        "df = pd.DataFrame(sampled_lines, columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p52zTek9oZR9"
      },
      "outputs": [],
      "source": [
        "LATVIAN_CHARS = 'A-Za-zĀČĒĢĪĶĻŅŠŪŽāčēģīķļņšūž'\n",
        "DIGITS = '0-9'\n",
        "PUNCTUATION = r'\\.\\,\\:\\;\\!\\?\\-\\(\\)\\'\\\"'\n",
        "\n",
        "df['text'] = (\n",
        "    df['text']\n",
        "    .astype(str)\n",
        "    .str.replace(r'\\r?\\n', ' ', regex=True)\n",
        "    .str.replace(r'[^' + LATVIAN_CHARS + DIGITS + PUNCTUATION + r'\\s]', '', regex=True)\n",
        "    .str.strip()\n",
        "    .str.replace(r'\\s+', ' ', regex=True)\n",
        ")\n",
        "\n",
        "df = df[df['text'].apply(lambda x: sum(w.isalnum() for w in x.split()) >= 3)]\n",
        "\n",
        "def truncate_words(text):\n",
        "    \"\"\" Atgiež nejaušu fragmentu 3-20 vārdu garumā no dotā teksta. \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 3:\n",
        "        return None\n",
        "    end = random.randint(3, min(20, len(words)))\n",
        "    return ' '.join(words[:end])\n",
        "\n",
        "df['text'] = df['text'].apply(truncate_words)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "df = df.sample(n=min(1000, len(df)), random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPkt6AVjox2S"
      },
      "outputs": [],
      "source": [
        "classified_wiki = df['text'].apply(classify)\n",
        "classified_wiki = pd.DataFrame(classified_wiki.tolist())\n",
        "classified_wiki = classified_wiki[[\"text\", \"level\"]]\n",
        "classified_wiki.to_csv(\"wiki_labeled.csv\", columns=[\"text\", \"level\"], index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz-GrPGieGyK"
      },
      "source": [
        "Eksāmenu tekstu sagatavošana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yI5ep73rkoy"
      },
      "outputs": [],
      "source": [
        "with open('exam_texts.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df_exams = pd.DataFrame(data[\"LV_EXAM_SET\"])[[\"TEKSTA_FRAGMENTS\"]]\n",
        "\n",
        "df_exams['TEKSTA_FRAGMENTS'] = (\n",
        "    df_exams['TEKSTA_FRAGMENTS']\n",
        "    .astype(str)\n",
        "    .str.replace(r'\\r?\\n', ' ', regex=True)\n",
        "    .str.replace(r'\\*', '', regex=True)\n",
        "    .str.strip()\n",
        "    .str.replace(r'\\s+', ' ', regex=True)\n",
        ")\n",
        "\n",
        "df_exams['text'] = df_exams['TEKSTA_FRAGMENTS']\n",
        "df_exams = df_exams.drop('TEKSTA_FRAGMENTS', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sC44rDitduv"
      },
      "outputs": [],
      "source": [
        "def until_first_excl_or_period(text):\n",
        "    \"\"\" Atgriež tekstu līdz pirmajam punktam vai izsaukuma zīmei dotajā tekstā. \"\"\"\n",
        "    for punct in ['.', '!']:\n",
        "        idx = text.find(punct)\n",
        "        if idx != -1:\n",
        "            return text[:idx + 1]\n",
        "    return text\n",
        "\n",
        "df_exams['text'] = df_exams['text'].apply(until_first_excl_or_period)\n",
        "\n",
        "df_exams = df_exams[\n",
        "    df_exams['text'].apply(lambda x: 2 <= len(str(x).split()) <= 20)\n",
        "].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw8QmrZguXQa"
      },
      "outputs": [],
      "source": [
        "classified_exams = df_exams['text'].apply(classify)\n",
        "classified_exams = pd.DataFrame(classified_exams.tolist())\n",
        "classified_exams.to_csv(\"exams_labeled.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuIwpSBeep5l"
      },
      "source": [
        "Tekstu apvienošana vienā datu kopā"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT1y8bINurOJ"
      },
      "outputs": [],
      "source": [
        "labeled_exams = pd.read_csv('exams_labeled.csv')\n",
        "labeled_wiki = pd.read_csv('wiki_labeled.csv')\n",
        "labeled_essays = pd.read_csv('essays_labeled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "379molx5vLM5"
      },
      "outputs": [],
      "source": [
        "labeled_full = pd.concat([labeled_exams, labeled_wiki, labeled_essays])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqwCaHI6vXpt"
      },
      "outputs": [],
      "source": [
        "labeled_full['text'] = (\n",
        "    labeled_full['text']\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r'^[\\s\\\"\\'\\-\\–\\.\\,\\:\\;\\!\\?\\(\\[]+', '', regex=True)\n",
        "    .str.replace(r'[\\s\\\"\\'\\-\\.\\–\\,\\:\\;\\!\\?\\)\\]]+$', '', regex=True)\n",
        ")\n",
        "\n",
        "labeled_full['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufoTCRZoxRh9"
      },
      "outputs": [],
      "source": [
        "final = labeled_full['text'].apply(classify)\n",
        "final = pd.DataFrame(final.tolist())\n",
        "final.to_csv(\"final.csv\", columns=['text','level'], index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxSQCn81x0fw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "final.groupby('level').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sākotnējie dati sagatavoti, vēlāk tie koriģēti manuāli."
      ],
      "metadata": {
        "id": "ijxb0oBXWzMK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "p5sBBm5LEulr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}